        \title[\whatshort, \simplenum, 
Slide \insertframenumber/\inserttotalframenumber ] {\what}
% this dirty hack allows me to display frame numbers in the footnotebar.

 

\subtitle{Class \simplenum: Prediction with Confidence}



\usepackage{graphicx}
%\subtitle{}

\begin{document}

\begin{frame}
  \titlepage

\begin{flushright}
\includegraphics[scale=0.4]{royalhollowaylogo}
~~~~~~~~~~~~~ 
\begin{minipage}[b]{0.22\linewidth}
{\footnotesize based on slides by 
V.~Vovk and A.~Gammerman}
\end{minipage}
\end{flushright}

\end{frame}

\begin{frame}
  \frametitle{Class Outline} \tableofcontents % You might wish to add
  %the option [pausesections]
\end{frame}


% Since this a solution template for a generic talk, very little can
% be said about how it should be structured. However, the talk length
% of between 15min and 45min and the theme suggest that you stick to
% the following rules:  

% - Exactly two or three sections (other than the summary).
% - At *most* three subsections per section.
% - Talk about 30s to 2min per frame. So there should be between about
%   15 and 30 frames, all told.

\begin{frame}[fragile]
\frametitle{References}

\begin{itemize}

\item[[VGS{]}] V.~Vovk, A.~Gammerman, G.~Shafer. 
``Algorithmic Learning in a Random World'', Springer, 2005.

\item[[SV{]}]  G.~Shafer and V.~Vovk.
``A Tutorial on Conformal Prediction''. Journal of Machine Learning Research 9 (2008) 371-421

\end{itemize}
\end{frame}

\section[Preliminaries]{Motivation and Preliminaries}

\begin{frame}\frametitle{Example: USPS}

\begin{itemize}

\item the problem is to label an image, which is a $16\times16$ matrix of pixels

--- it is known that an image represents a hand-written
digit, from 0 to 9

\item we are given a training set containing a large
number of labelled images

--- USPS dataset: scanned zip codes from envelopes

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Training Data}

\begin{center}
  \includegraphics[width=7cm, keepaspectratio]{img103.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Test Example}
\begin{center}
  \includegraphics[width=5cm, keepaspectratio]{problem3}
\end{center}
\begin{itemize}
\item if we drop all \alert{unlikely} classifications, 3 and 5 remain

\item we may want to output both 3 and 5: both are possible
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Batch Setup}

\begin{itemize}
\item we are given a \alert{training set} of \alert{labelled examples}
  $(x_1,y_1),(x_2,y_2),\ldots,(x_{n-1},y_{n-1})$

--- every example $z_i=(x_i,y_i)$ consists of an \alert{object} $x_i$
     and its \alert{label} $y_i$.

--- let $X$ be the set of possible objects, $Y$ the set of possible
    labels, and $Z=X\times Y$ the set of all labelled examples

\item we are also given test objects $x$ and need to say something
     about their labels

--- assuming that all the the examples were generated by the same
  mechanism
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{On-line Setup}

\begin{itemize}
\item on step $T$ we receive an object $x_T$ and should say something
  about its label; then we see the true label

\item one can say that on step $T$ we have the training set 
$(x_1,y_1),\ldots,(x_{T-1},y_{T-1})$ and the test object $x_T$

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Point and Region Prediction}

\begin{itemize}
\item a \alert{point prediction} is a single label

\item a \alert{region prediction} is a set of labels

--- recall confidence intervals; the true value is likely to be one of
    the points from an interval

\item region predictions make perfect sense for various applications;
    think of medical diagnosis

\item a region prediction is correct if the true label is inside the set

--- a prediction is an error if the true label is not in the set

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Measuring the Quality of a Region Prediction}

\begin{itemize}
\item the \alert{accuracy} of a predictor (prediction algorithm) is
the number (or the fraction) of correct predictions it has made

--- there is a trivial way to get 100\% accuracy with region
    predictors: always predict the set of all possible labels

\item the \alert{efficiency} is concerned with the size of 
    prediction regions; an efficient predictor outputs small
    sets

--- there is a trade-off between accuracy and efficiency

\item fix a \alert{confidence level} $\delta$; we want to get maximum
efficiency while keeping the fraction of errors at $1-\delta$ or less

--- a predictor making  $1-\delta$ or fewer errors is
    called \alert{valid} at the confidence level of $\delta$

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Idea}

\begin{itemize}
\item we will reject unlikely labels

\item what is an unlikely label?

\item let us revise hypothesis testing; when do we reject a hypothesis $H_0$?

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Hypothesis Testing (1)}

\begin{itemize}

\item suppose that we have some data $D$ and want to know whether it
    is consistent with a \alert{null hypothesis} $H_0$

--- if the data is likely under $H_0$, we keep $H_0$; otherwise we
reject $H_0$ 

\item take a \alert{test statistic} (a function on the data) $T$ 

\item take sets $E_\alpha$ such that $\Pr(T(D)\in E_\alpha\mid H_0)=\alpha$
and $E_{\alpha_1}\subseteq E_{\alpha_2}$ if $\alpha_1>\alpha_2$

--- interpretation: $E_\alpha$ contains extreme values of $T(D)$; the
    probability of seeing something as extreme as $E_\alpha$ is
    $\alpha$

--- usually $E_\alpha$ is the sets $\{T\mid T\ge t_\alpha\}$

\item then the \alert{p-value} is the minimum $\alpha$ such that
$T(D)\in E_\alpha$

--- in other words, the p-value is the probability under $H_0$ that we
    see something as extreme as or more extreme than $D$

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Hypothesis Testing (2)}

\begin{itemize}

\item usually we pick a \alert{significance level} $\varepsilon$
    (typically 1\% or 5\%) and reject the hypothesis if the p-value
    is less than or equal to $\varepsilon$

--- \alert{confidence level} $\delta = 1-\varepsilon$

\item the p-value is the probability of the following error: given
    that $H_0$ was right, we rejected it because of unlikely data

--- it is not the probability that $H_0$ is true

\item the choice of a statistic $T$ and sets $E_\alpha$ (i.e., the
formalisation of ``extreme'') is arbitrary; some are more useful than
others for particular problems

--- for example, if we have a natural competing hypothesis $H_1$, then
    ``extreme'' should mean ``likely by $H_1$ and unlikely by $H_0$''
    (cf.~Neyman-Pearson lemma)

\end{itemize}
\end{frame}



\section[TCP]{Transductive Conformal Prediction}


\begin{frame}\frametitle{Nonconformity Measure}

\begin{itemize}
\item let $Z$ be the set of all possible labelled examples and
    $Z^{(*)}$ be the set of all \alert{bags} of examples

--- a bag is like a set in that it has no order on its element, but it
    can contain several copies of the same element

\item a \alert{nonconformity measure (score)} is a function
$$A: Z^{(*)}\times Z\to\R\cup\{-\infty,+\infty\}$$

--- it tells us how different a labelled example $z$ is from a bag
    $B$


\end{itemize}

\end{frame}


\begin{frame}\frametitle{Nonconformity Measure: Intuition}

\includegraphics[width=7cm, keepaspectratio]{knn.jpg}

\begin{itemize}
\item nonconformity measure quantifies strangeness
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Specific Nonconformity Measures}

\begin{itemize}
\item the choice of a nonconformity measure is arbitrary (as is the
    choice of a kernel in regression or the choice of a statistic in
    hypothesis testing); good meaningful measures lead to better
    results

\item often nonconformity measures are inspired by learning algorithms

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Specific Nonconformity Measures: NN}

\begin{itemize}

\item nearest neighbour-inspired measure for multiclass classification
    (i.e., the case with finitely many possible labels):

--- for a bag

$B = \Lbag (x_1,y_1), (x_2,y_2),\ldots, (x_{n},y_{n})\Rbag$
and $z=(x,y)$

$$A(B,z) = \frac{\min_{j: y_j = y} d(x_j,x)}{\min_{j: y_j \ne
y}d(x_j,x)}$$

where $d$ is Euclidean distance

\item it is the ratio

$$
\frac{\text{distance to $z$'s nearest neighbour in $B$ with the same
label}}{\text{distance to $z$'s nearest neighbour in $B$ with a different
label}}
$$

\end{itemize}
\end{frame}


\begin{frame}\frametitle{Specific Nonconformity Measures: Linear Regression}

\begin{itemize}

\item for the case of real-valued labels

\item for a bag $B = \Lbag (x_1,y_1), (x_2,y_2),\ldots,
    (x_{n},y_{n})\Rbag$ and a labelled example $z = (x,y)$

--- fit a regression line $y=ax+b$ (or a hyperplane $y=\langle
    a,x\rangle+b$ if $x$s are multidimensional)

--- calculate the regression prediction for $x$: $\hat y = ax+b$ (or 
$\hat y=\langle a,x\rangle+b$)

--- take $A(B,z) = (y-\hat y)^2$

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Specific Nonconformity Measures: Logistic Regression}

\begin{itemize}

\item for the case of binary labels 0 or 1

\item for a bag $B = \Lbag (x_1,y_1), (x_2,y_2),\ldots,
    (x_{n},y_{n})\Rbag$ and a labelled example $z = (x,y)$

--- fit a logistic function $y=1/(1+e^{-(\langle
    w,x\rangle+b)})$ 

--- take

$$
A(B,z) = 
\begin{cases}
\frac{1}{1+e^{-(\langle
    w,x\rangle+b)}} & \text{~~if $y=0$}\\
\frac{1}{1+e^{\langle
    w,x\rangle+b}} & \text{~~if $y=1$}\\
\end{cases}
$$

\end{itemize}
\end{frame}


\begin{frame}\frametitle{P-values (1)}

\begin{itemize}

\item suppose that we have a training set
$B = ((x_1,y_1),(x_2,y_2),\ldots,(x_{n-1},y_{n-1}))$ and want to label a new
object $x_n$

--- we use a nonconformity measure $A$

\item consider a candidate label $y$

\item we can calculate the non-conformity measure $\alpha_n=A(B,(x_n,y))$

--- but this number only makes sense in comparison to non-conformity of
    other examples!

\end{itemize}
\end{frame}


\begin{frame}\frametitle{P-values (2)}

\begin{itemize}

\item let us take $B' =
    ((x_1,y_1),(x_2,y_2),\ldots,(x_{n-1},y_{n-1}),(x_n,y))$ and for
    every $i = 1,2,\ldots,n-1$ calculate
$$
\alpha_i = A(B'\setminus \Lbag (x_i,y_i)\Rbag, (x_i,y_i))
$$

--- $\alpha_i$ says how strange $(x_i,y_i)$ is compared to other
    examples (including the new provisional $(x_n,y)$)

\item let the p-value of the label $y$ be
\begin{multline*}
p_y =\frac{|\{i\in\{1,2,\ldots,n\}:\alpha_i\ge\alpha_n\}|}{n}= \\
\hspace*{-0.5cm}\frac{\text{number of examples that conform worse or the same as $(x_n,y)$}}{n}
\end{multline*}

\item let us calculate $p_y$ for every possible label $y$

\end{itemize}
\end{frame}


\begin{frame}\frametitle{Point Predictor with Confidence and Credibility}

\begin{itemize}

\item let $p_y$ achieves the maximum at $y_0$

--- if $x_0$ is labelled with $y_0$, it look least strange

\item let us output $y_0$ as the prediction and say that $p_{y_0}$ is
the \alert{credibility}

\item let $y_1$ be the value such that $p_{y_1}$ is the second largest

--- then $1-p_{y_1}$ is called the \alert{confidence}

\item ideally we want $p_{y_0}$ to be close to 1 and $p_{y_1}$ close to 0

--- credibility shows how likely this label is

--- confidence shows how sure we are it is this label and not another
    one

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Example: Classification on USPS}

\begin{itemize}

\item each line shows p-values for a particular case

--- a non-conformity measure based on Support Vector Machines (SVM)
    was used

--- the p-values are in \%

\end{itemize}
% with the polynomial kernel of degree 5.
\begin{center}
\tiny
%\begin{table}
\hspace*{-0.5cm}\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
0& 1 & 2& 3& 4 &5 & 6 & 7 & 8 & 9 &largest&2nd& Conf & Cred\\
\hline
0.01 & 0.11 & 0.01 &0.01 &  0.07 & 0.01 & 100 & 0.01 & 0.01 & 0.01 & 6 & 1 & 99.89 & 100 \\
\hline
0.32 & 0.38 & 1.07 & 0.67 & 1.43 & 0.67 & 0.38 & 0.33 & 0.73 & 0.78 & 4 & 2 & 98.93 & 1.43 \\
\hline
0.01&0.27&0.03&0.04&0.18&0.01&0.04&0.01&0.12 & 100 & 9 &1 &99.73 & 100 \\
\hline
\end{tabular}
\end{center}
%\end{table}
\normalsize

\end{frame}

\begin{frame}\frametitle{Region Prediction}

\begin{itemize}

\item take significance level $\varepsilon>0$

\item include in the region prediction all labels $y$ with
$p_y>\varepsilon$

--- labels with low p-values $p_y\le \varepsilon$ are rejected

\item $\Gamma^\varepsilon = \{y : p_y>\varepsilon\}$

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Example: Classification of Digits}

\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
0& 1 & 2& 3& 4 &5 & 6 & 7 & 8 & 9\\
\hline
0.8 & 0.3 & 0.2 & 0.7 & 0.9 & 0.4&
 0.6 & 0.7 & 0.8 & 0.5\\
\hline
\end{tabular}

\begin{itemize}

\item  confidence level 15\% (significance 85\%): 
$ \Gamma^{0.85} = \{ 4 \}$

\item confidence level 25\% (significance 75\%)
$\Gamma^{0.75} = \{ 0, 4, 8\}$

\item confidence level 35\% (significance 65\%)
$\Gamma^{0.65} = \{ 0, 3, 4, 7, 8 \}$

\item confidence level 95\% (significance 5\%)
$\Gamma^{0.05} = \{ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 \}$

\end{itemize}
\end{frame}


\begin{frame}\frametitle{Region Prediction and Errors}

\begin{itemize}

\item a region prediction makes an error if the true label is not in
the region: $y_n\notin\Gamma^\varepsilon$

\item size of the region:

--- empty prediction: $|\Gamma^\varepsilon| = 0$; such predictions
    always lead to errors

--- certain prediction: $|\Gamma^\varepsilon| = 1$; the ideal case

--- uncertain predictions:  $|\Gamma^\varepsilon| > 1$; most common

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Validity}

\begin{itemize}

\item TCP region predictor is valid if the examples are drawn from an
exchangeable distribution

\item they are valid in the sense that the error probability does not exceed
    $\varepsilon$ (Vovk)

--- this follows from a more precise property that will be formulated later

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Exchangeability}

\begin{itemize}

\item a sequence of random variables $Z_1,Z_2,\ldots$
is \alert{exchangeable} if for any $n$ and any permutation $\sigma$ on
$1,2,\ldots,n$ the distribution of $Z_1,\ldots,Z_n$ is the same as the
distribution of $Z_{\sigma(1)},\ldots,Z_{\sigma(n)}$

--- this applies to labelled examples $(x_t,y_t)$; the objects $x_t$
    have to be stochastic too

\item any i.i.d.\ sequence is exchangeable, but not every exchangeable
    sequence is i.i.d.

--- exchangeability is a weaker property

\end{itemize}
\end{frame}

\begin{frame}\frametitle{Exchangeability vs i.i.d.}

\begin{itemize}

\item let $Z_1$ and $Z_2$ take values 0 or 1; their joint distribution
can be described with a contingency table:
$$
\hspace*{-1.5cm}\begin{array}{lccl}
& \multicolumn{2}{c}{\text{iid}}  &\\
        & Z_1 = 0 & Z_1 = 1 &     \\
Z_2 = 0 & 0.36    & 0.24    & 0.6 \\
Z_2 = 1 & 0.24    & 0.16    & 0.4 \\
        & 0.6     & 0.4     &
\end{array}~~~~~\begin{array}{lccl}
& \multicolumn{2}{c}{\text{exchangeable}}  &\\
        & Z_1 = 0 & Z_1 = 1 &     \\
Z_2 = 0 & 0.5    & 0.2    & 0.7 \\
Z_2 = 1 & 0.2    & 0.1    & 0.3 \\
        & 0.7    & 0.3    &
\end{array}
$$

\item it is sufficient for exchangeability that $\Pr(Z_1=0,
        Z_2=1) = \Pr(Z_1=1, Z_2=0)$, i.e., the matrix is symmetric

\item i.i.d.\ requires that joint probabilities should be products,
$\Pr(Z_1=a, Z_2=b)=\Pr(Z_1=a)\Pr(Z_2=b)$

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{On-line Prediction for USPS, 95\% Confidence}

\begin{center}
  \includegraphics[width=7.5cm, keepaspectratio]{TCM0_95.pdf}
\end{center}
\begin{itemize}
\item the predictions are made on the 95\% confidence level
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{On-line Prediction for USPS, 99\% Confidence}

\begin{center}
  \includegraphics[width=7.5cm, keepaspectratio]{TCM0_99.pdf}
\end{center}
\begin{itemize}
\item the predictions are made on the 99\% confidence level
\end{itemize}

\end{frame}



\begin{frame}
\frametitle{Optimality of TCP Region Predictors (1)}

\begin{itemize}
\item the region predictors we have constructed have the following
properties:

\begin{enumerate}
\item the predictions are invariant w.r.t.\ the ordering of the old
examples; indeed, if we change the order of the training examples, the
values of $\alpha$ will be shuffled but stay the same and all p-values
stay the same

\item the probability of an error does not exceed the specified
significance level $\varepsilon$

\item the prediction regions are nested: if
$\varepsilon_1\ge\varepsilon_2$, then
$\Gamma^{\varepsilon_1}\subseteq\Gamma^{\varepsilon_2}$ 

\end{enumerate}


\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Optimality of TCP Region Predictors (2)}

\begin{itemize}
\item let a region predictor makes predictions $\Gamma^\varepsilon$
satisfying these three properties

\item then there is a non-conformity measure $A$ such that that 
 $\Gamma^\varepsilon\supseteq\Gamma^\varepsilon_A$, where
 $\Gamma^\varepsilon_A$ is the TCP region predictor generated by $A$

\item any region predictor is a  TCP region predictor worsened
 
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{Randomised TCP Region Predictor (1)}

\begin{itemize}
\item the \alert{randomised} (\alert{smoothed}) predictor clarifies
 some theoretical properties of the TCP Region Predictor

--- not so practical...

\item recall the formula for the p-value: $p_y
=|\{i\in\{1,\ldots,n\}:\alpha_i\ge\alpha_n\}|/n$

--- the formula includes ``$\ge$''; the case ``$=$'' is not very common in
    practice but possible

\item let us sample random $\tau_n$ from the uniform distribution on
    $[0,1]$ and let

$$
\tilde p_y =\frac{|\{i\in\{1,\ldots,n\}: \alpha_i>\alpha_n\}|+\tau_n|\{i\in\{1,\ldots,n\}: \alpha_i=\alpha_n\}|}{n}
$$

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Randomised TCP Region Predictor (2)}

\begin{itemize}
\item define region predictor $\tilde\Gamma^\varepsilon = \{y :
\tilde p_y>\varepsilon\}$

\item since $\tilde p_y\le p_y$, we have
$\tilde\Gamma^\varepsilon \subseteq\Gamma^\varepsilon$

--- the randomised predictor produces regions that are never larger
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Randomised TCP Region Predictor: Optimality}

\begin{itemize}
\item if the distribution is exchangeable, the randomised predictor
is \alert{strictly valid} in the on-line mode:

--- the probability of the error equals $\varepsilon$

--- the errors on different steps are i.i.d.

\item let the sequence of zeros and ones
$0,0,1,0,\ldots$ show whether the randomised TCP region predictor made
an error (1 means error and 0 means no error)

--- then it is a sequence of i.i.d.\ Bernoulli trials with the
    probability of 1 equal to $\varepsilon$


\end{itemize}
\end{frame}


\section[ICP]{Inductive Conformal Predictor}

\begin{frame}
\frametitle{Motivation}
\begin{itemize}
\item the TCP predictors are time-consuming

--- let the training set size be $n-1$; then for every label we need
    to calculate $n$ non-conformity measures $\alpha$

--- suppose that we use a (linear or logistic) regression-based
    non-conformity measure

--- to work out each $\alpha_i$ we train a regression model

--- thus we need $n$ models per possible label

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Decision Rule}
\begin{itemize}

\item suppose that we have a training set of labelled examples
  $(x_1,y_1),(x_2,y_2),\ldots,(x_{n-1},y_{n-1})$ and need to make a
  conformal prediction for $x_n$

\item let us partition the training set into a
training part of size $n-m$ and a calibration part of size $m-1$

\item on the training part we train a decision rule (such as logistic
regression) $D$
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{P-values}
\begin{itemize}

\item take a loss function (usually called discrepancy measure in this
context) $\Delta$

\item Let $(x_{k_1},y_{k_1}),\ldots,(x_{k_{m-1}},y_{k_{m-1}})$ be the
calibration set; for each element calculate the non-conformity
$\alpha_i = \Delta(D(x_{k_i}),y_{k_i})$ ($i=1,2,\ldots,m-1$)

--- and $\alpha_m = \Delta(D(x_n),y)$, where $y$ is a candidate label

\item then on the basis of $\alpha_1,\ldots,\alpha_m$ we can calculate
the p-value as before:
$$
p_y =\frac{|\{i\in\{1,2,\ldots,m\}:\alpha_i\ge\alpha_m\}|}{m}
$$

--- or randomised (smoothed)
$$
\hspace*{-0.5cm}\tilde p_y =\frac{|\{i\in\{1,\ldots,m\}: \alpha_i>\alpha_m\}|+\tau_n|\{i\in\{1,\ldots,m\}: \alpha_i=\alpha_m\}|}{m}
$$


\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Prediction}
\begin{itemize}

\item we can use p-values to make

--- point predictions with confidence and credibility

--- region predictions

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Validity}
\begin{itemize}

\item validity holds in the on-line mode in the following setup

--- provided $(x_t,y_t)$ come from an exchangeable distribution

\item take an increasing sequence of integers $k_1<k_2<...$ (the
sequence may be finite and finish with $k = +\infty$)

\item for time $T$:

--- if $T\le k_1$ just use a standard randomised TCP predictor with
    significance level $\varepsilon$

--- otherwise let $i$ be the largest such that $k_i<T$ (i.e.,
    $k_i<T\le k_{i+1}$)

--- use the first $k_i$ examples as the training set and the remaining
    $T-k_i-1$ as the calibration set

--- make a randomised prediction with
    significance level $\varepsilon$

\item then the probability of error equals $\varepsilon$ and errors
    are independent

\end{itemize}
\end{frame}

\section[Mondrian]{Mondrian Predictor}


\begin{frame}
\frametitle{Motivation}
\begin{itemize}

\item the true labels may follow a very unbalanced pattern

\item for example, consider the problem of medical diagnosis; some
diseases are common (common cold) and some are rare (lupus)

--- so we can altogether ignore rare illnesses and still get the
    diagnosis right in the vast majority of cases

\item the guarantees for prediction with confidence we have considered
    so far apply to the overall performance and may be weak on
    particular labels

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Mondrian Taxonomy}
\begin{itemize}

\item suppose that we have a finite set of categories $K$

\item a labelled example $(x_t,y_t)$ can be classified as belonging to
    a category on the basis of:

--- object $x_t$

--- label $y_t$

--- time $t$

\item a \alert{Mondrian taxonomy} $\kappa: Z\times \mathbb N\to K$ is a mapping
from labelled examples and their numbers to $K$

--- it is requires that the set of elements of every category
    $\kappa^{-1}(k)\subseteq  Z\times \mathbb N$ is a rectangle
    $A\times B\subseteq  Z\times \mathbb N$

--- hence the name

\item i.e., categories can change with time, but subject to a condition

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Piet Mondrian}
\includegraphics[width=6cm, keepaspectratio]{pm.jpg}

\end{frame}


\begin{frame}
\frametitle{P-values}
\begin{itemize}

\item we calculate p-values taking classification into account

\item suppose that we need to classify $x_T$ and we consider a label
$y$

--- let the category be $\kappa((x_T,y),T)=\kappa_T$

--- the categories of the past examples be $\kappa((x_t,y_t),t)=\kappa_t$

\item non-conformity scores may depend on categories

\item then we calculate p-values as follows:
$$
p_y =\frac{|\{t\in\{1\ldots,T\}: \kappa_t = \kappa_T
\text{~and~}\alpha_t\ge\alpha_T\}|}{|\{t\in\{1\ldots,T\}: \kappa_t = \kappa_T\}|}
$$

--- randomised (smoothed) $\tilde p_y=$
{\small
$$
\hspace*{-1.3cm}\frac{|\{t\in\{1,\ldots,T\}: \kappa_t = \kappa_T,
\alpha_t>\alpha_T\}|+\tau_T|\{t\in\{1,\ldots,T\}: \kappa_t
= \kappa_T, \alpha_t=\alpha_T\}|}{|\{t\in\{1\ldots,T\}: \kappa_t = \kappa_T\}|}
$$
}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Validity}
\begin{itemize}

\item in the on-line mode randomised Mondrian region predictor is
valid category-wise 

--- provided $(x_t,y_t)$ come from an exchangeable distribution

\item the probability of an error for an example from every category
equals the confidence level

\item errors are independent

\end{itemize}
\end{frame}

\end{document}










